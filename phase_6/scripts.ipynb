{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a492da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            d_model: モデルの隠れ層の次元数\n",
    "            dropout: ドロップアウト率\n",
    "            max_len: 想定される入力シーケンス最大長\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Positional Encoding行列[max_len, d_model]の初期化\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # 位置情報のベクトル\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # 10000^(2i/d_model)の計算\n",
    "        # 対数空間で計算してからexpで戻すことで数値安定性を確保\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # 偶数次元にsin、奇数次元にcosを適用\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # バッチ次元を追加してshapeを[1, max_len, d_model]に変形\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # モデルのパラメータとして登録（学習されない）\n",
    "        # state_dictに保存されるが、勾配計算optimizerの対象にはならない\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Enbeddingされた入力テンソル、shapeは[batch_size, seq_len, d_model]\n",
    "        \n",
    "        Returns:\n",
    "            Positional Encodingが加算されたテンソル、shapeは[batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # 入力テンソルの長さに合わせてPositional Encodingをスライスして加算\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "        # ドロップアウトを適用して出力\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): モデルの隠れ層の次元数\n",
    "            num_heads (int): ヘッドの数\n",
    "            dropout (float, optional): ドロップアウト率. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # d_modelがnum_headsで割り切れることを確認\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # 各ヘッドの次元数\n",
    "\n",
    "        # Q, K, Vの線形変換\n",
    "        # 実際には全ヘッド分を一度に計算するため、出力次元はd_modelのまま\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): [batch_size, seq_len, d_model]\n",
    "            key (torch.Tensor):   [batch_size, seq_len, d_model]\n",
    "            value (torch.Tensor): [batch_size, seq_len, d_model]\n",
    "            mask (torch.Tensor, optional): [batch_size, 1, 1, seq_len] または [batch_size, 1, seq_len, seq_len]\n",
    "                                           (0: マスクなし、1: マスクありなどの定義によるが、ここでは加算マスクを想定)\n",
    "                                           Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. 線形変換\n",
    "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, num_heads]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "\n",
    "        # 2. ヘッドの分割\n",
    "        # [batch_size, seq_len, num_heads] -> [batch_size, seq_len, num_heads, d_k]\n",
    "        # その後、計算しやすいようにヘッドの次元を先頭に移動させる(転置) -> [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Scaled Dot-Product Attention\n",
    "        # 3.1. スコアの計算 Q * K^T / sqrt(d_k)\n",
    "        # Q: [..., seq_len_q, d_k], K^T: [..., d_k, seq_len_k] -> scores: [..., seq_len_q, seq_len_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 3.2. マスクの適用(optional)\n",
    "        if mask is not None:\n",
    "            # ここではマスクが0の場所を非常に小さい値(-1e9)でマスクすると仮定\n",
    "            # 実装により1と0の定義が異なる場合があるため注意する\n",
    "            # 非常に小さい値(-1e9)で埋めることで、softmax後にほぼ0になるようにする\n",
    "            # scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            scores = scores + mask # 加算マスクの場合 (maskが0の場所に-1e9が入っている想定)\n",
    "        \n",
    "        # 3.3. softmax & dropout\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 3.4. Valueとの積\n",
    "        # attention_weights: [..., seq_len_q, seq_len_k] * V: [..., seq_len_k, d_k] -> [...,seq_lenn_q, d_k]\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 4. ヘッドの結合\n",
    "        # [batch_size, num_beads, seq_len, d_k] -> [batch_size, seq_len, num_heads, d_k]\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # [batch_size, seq_len, d_model]に戻す\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 5. 線形変換\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): モデルの次元数\n",
    "            d_ff (int): FFNの中間層の次元数\n",
    "            dropout (float, optional): ドロップアウト率. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 一層目 d_model -> d_ff\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # 二層目 d_ff -> d_model\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # ドロップアウト\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 活性化関数 ReLU\n",
    "        # 元論文ではReLUが使われているが、近年のLLMではGELUがよく使われている\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch_size, seq_len, d_model]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Linear -> ReLU -> Dropout -> Linear\n",
    "        # x: [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_ff]\n",
    "        hidden = self.activation(self.w_1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        # [batch_size, seq_len, d_ff] -> [batch_size, seq_len, d_model]\n",
    "        output = self.w_2(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76feae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Self-Attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # 2. Feed-Forward Network layer\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # 3. Layer Normalization & Dropout layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch_size, seq_len, d_model]\n",
    "            mask (torch.Tensor, optional): Padding Maskなど. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Sublayer 1: Self-Attention\n",
    "        # Residual Connection: x + Sublayer(x)\n",
    "        # Post-LN: Norm(x + Sublayer(x))\n",
    "        # Attentionの入出力は同じshape\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # 2. Sublayer 2: Feed-Forward Network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Masked Self-Attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # 2. Cross-Attention layer (Source-Target Attention)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        # 3. Feed-Forward Network layer\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        # 4. Normalization & Dropout layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Decoderへの入力テンソル、shapeは[batch_size, tgt_len, d_model]\n",
    "            memory (torch.Tensor): Encoderの出力テンソル、shapeは[batch_size, src_len, d_model]\n",
    "            src_mask (torch.Tensor): Memoryに対するマスク(Padding Mask)\n",
    "            tgt_mask (torch.Tensor): Self-Attention用のマスク(Look-Ahead Mask + Padding Mask)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Decoderの出力テンソル、shapeは[batch_size, tgt_len, d_model]\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Sublayer 1: Masked Self-Attention\n",
    "        # 未来の単語を見ないように tgt_mask を適用\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "\n",
    "        # 2. Sublayer 2: Cross-Attention\n",
    "        # Query = x(Decoderの出力), Key = Value = memory(Encoderの出力)\n",
    "        # Encoder側のパディングを見ないように src_mask を適用\n",
    "        attn_output = self.cross_attn(x, memory, memory, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        # 3. Sublayer 3: Feed-Forward Network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm3(x + self.dropout(ffn_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, d_ff: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
    "\n",
    "        # EncoderLayerをnum_layers個積み重ねる\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Embedding & Positional Encoding\n",
    "        # 論文通り sqrt(d_model)でスケーリング\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # 2. Apply all layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, d_ff: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self. pos_encoding = PositionalEncoding(d_model, dropout, max_len)\n",
    "\n",
    "        # DecoderLayerをnum_layers個積み重ねる\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        # 1. Embedding & Positional Encoding\n",
    "        # 論文通り sqrt(d_model)でスケーリング\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # 2. Apply all layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size: int, tgt_vocab_size: int, d_model: int = 512, num_layers: int = 6, num_heads: int = 8, d_ff: int = 2048, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, max_len, dropout)\n",
    "\n",
    "        # 最終出力層の線形変換(Linear Projection)\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor = None, tgt_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): [batch, src_len] Encoderへの入力単語ID列\n",
    "            tgt (torch.Tensor): [batch, tgt_len] Decoderへの入力単語ID列\n",
    "            src_mask (torch.Tensor, optional): Encoder用のマスク. Defaults to None.\n",
    "            tgt_mask (torch.Tensor, optional): Decoder用のマスク. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch, tgt_len, tgt_vocab_size] 出力単語の確率分布\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Encode\n",
    "        # memory: [batch, src_len, d_model]\n",
    "        memory = self.encoder(src, src_mask)\n",
    "\n",
    "        # 2. Decode\n",
    "        # decoder_output: [batch, tgt_len, d_model]\n",
    "        decoder_output = self.decoder(tgt, memory, src_mask, tgt_mask)\n",
    "\n",
    "        # 3. Final linear layer\n",
    "        # logits: [batch, tgt_len, tgt_vocab_size]\n",
    "        logits = self.fc_out(decoder_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"推論時にEncoderのみを動かすためのヘルパー\"\"\"\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, src_mask: torch.Tensor = None, tgt_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"推論時にDecoderのみを動かすためのヘルパー\"\"\"\n",
    "        return self.decoder(tgt, memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07052e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"パディングマスクを作成する関数。<pad>の部分を1e-9、それ以外を0にする。\n",
    "\n",
    "    Args:\n",
    "        seq (torch.Tensor): [batch_size, seq_len]の形状を持つテンソル。入力単語のID列\n",
    "        pad_idx (int, optional): パディングを表すID。 Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [batch_size, 1, 1, seq_len] Mult-head Attentionのスコアに加算するためのマスク\n",
    "    \"\"\"\n",
    "\n",
    "    # seq == pad_idx の部分はTrue、それ以外はFalse\n",
    "    mask = (seq == pad_idx)\n",
    "\n",
    "    # shapeを [batch_size, 1, 1, seq_len] に変形\n",
    "    # Trueを1e-9、Falseを0に変換\n",
    "    # float型に変換しないと加算時にエラーになる\n",
    "    return mask.unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"未来の単語を見えなくするための上三角マスクを作成する関数\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): シーケンス長\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [1, 1, seq_len, seq_len] 対角成分より上が1e-9、それ以外が0の行列\n",
    "    \"\"\"\n",
    "    # torch.triuで上三角行列を取り出す (diagonal=1で対角線のひとつ上から)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "    # マスク部分を1e-9、それ以外を0に変換\n",
    "    return mask.unsqueeze(0).unsqueeze(0).float() * -1e9\n",
    "\n",
    "\n",
    "def create_masks(src: torch.Tensor, tgt: torch.Tensor, pad_idx: int = 0):\n",
    "    \"\"\"EncoderとDecoderに必要なすべてのマスクを一括生成するヘルパー関数\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): [batch_size, src_len]\n",
    "        tgt (torch.Tensor): [batch_size, tgt_len]\n",
    "        pad_idx (int, optional): パディングID。 Defaults to 0.\n",
    "    \"\"\"\n",
    "    # 1. Encoder用のマスク(Padding Maskのみ)\n",
    "    src_mask = create_padding_mask(src, pad_idx)\n",
    "\n",
    "    # 2. Decoder用のマスク(Padding Mask + Look-ahead Mask)\n",
    "    # 2.1. TargetのPadding Mask [batch_size, 1, 1, tgt_len]\n",
    "    tgt_pad_mask = create_padding_mask(tgt, pad_idx)\n",
    "\n",
    "    # 2.2. Look-ahead Mask [1, 1, tgt_len, tgt_len]\n",
    "    tgt_len = tgt.size(1)\n",
    "    look_ahead_mask = create_look_ahead_mask(tgt_len).to(tgt.device)\n",
    "\n",
    "    # 2.3. Padding MaskとLook-ahead Maskを結合 (どちらかが-1e9なら-1e9になるように加算またはmaxを取る)\n",
    "    # ここで単純に和を取ると-2e9になる箇所ができるが、Softmaxにおいては十分小さいので計算には影響しない\n",
    "    # 論理和(OR)的にマスクしたいので、最小値を取る実装もよくある(torch.min)\n",
    "    tgt_mask = torch.min(tgt_pad_mask, look_ahead_mask)\n",
    "\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "\n",
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, d_model: int, warmup_steps: int = 4000):\n",
    "        \"\"\"Transformer用学習率スケジューラ (Noam Scheduler)\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): PytorchのOptimizer\n",
    "            d_model (int): モデルの次元数\n",
    "            warmup_steps (int, optional): ウォームアップステップ数。 Defaults to 4000.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"1ステップ進め、学習率を更新し、optimizer.step()を実行する\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "\n",
    "        # Optimizerの学習率を更新\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        \"\"\"現在のステップ数に基づき学習率を計算\n",
    "\n",
    "        Returns:\n",
    "            float: 学習率\n",
    "        \"\"\"\n",
    "        step = self.current_step\n",
    "        return (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"optimizerの勾配を初期化(ゼロにする)\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b42bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, data_loader: list, optimizer: torch.optim.Optimizer, criterion: nn.Module, device: torch.device) -> float:\n",
    "    \"\"\"1エポック分の学習を行う関数\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): transformerモデル\n",
    "        data_loader (list): バッチデータのリスト\n",
    "        optimizer (torch.optim.Optimizer): オプティマイザ (NoamSchedulerでラップされている想定)\n",
    "        criterion (nn.Module): 損失関数 (LabelSmoothingCrossEntropy)\n",
    "        device (torch.device): 計算デバイス(CPU or GPU)\n",
    "\n",
    "    Returns:\n",
    "        float: 1エポック分の平均損失\n",
    "    \"\"\"\n",
    "    model.train() # 学習モード(Dropout有効)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src, tgt = batch\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # targetの処理\n",
    "        # device_input: <sos>, w1, w2, ... (最後を含まない)\n",
    "        # target_label: w1, w2, ..., <eos> (最初を含まない)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_label = tgt[:, 1:]\n",
    "\n",
    "        # マスクの作成\n",
    "        # tgt_inputを使ってマスクを作る点に注意\n",
    "        src_mask, tgt_mask = create_masks(src, tgt_input, pad_idx=0)\n",
    "\n",
    "        # Forward pass\n",
    "        # output: [batch_size, tgt_len, vocab_size]\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        # Loss計算\n",
    "        # CrossEntropyLoss は [N, C] の入力を期待するため、出力とラベルを変形\n",
    "        # output: [batch_size * tgt_len, vocab_size]\n",
    "        # label: [batch_size * tgt_len]\n",
    "        loss = criterion(output.contiguous().view(-1, output.size(-1)), tgt_label.contiguous().view(-1))\n",
    "\n",
    "        # Backward & Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() # NoamScheduler.step()内でoptimizer.step()が呼ばれる\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f3d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model: nn.Module, src: torch.Tensor, src_mask: torch.Tensor, max_len: int, start_symbol: int, end_symbol: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Greedy Decodingによる推論を行う関数\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 学習済みTransformerモデル\n",
    "        src (torch.Tensor): [1, src_len] エンコードする入力シーケンス\n",
    "        src_mask (torch.Tensor): [1, 1, 1, src_len] エンコード入力に対するマスク\n",
    "        max_len (int): 最大生成長\n",
    "        start_symbol (int): 開始トークン<sos>のID\n",
    "        end_symbol (int): 終了トークン<eos>のID\n",
    "        device (torch.device): 計算デバイス(CPU or GPU)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [1, gen_len] 生成されたシーケンスのトークンID列\n",
    "    \"\"\"\n",
    "    model.eval() # 推論モード(Dropout無効)\n",
    "\n",
    "    # 1. Encode(一度だけ実行)\n",
    "    # memory: [1, seq_len, d_model]\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # 2. Decode (ループ処理)\n",
    "    # 生成シーケンスの初期化 (開始トークン <sos> のみ)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        # 現在のysに対するマスクを作成\n",
    "        tgt_mask = create_look_ahead_mask(ys.size(1)).to(device)\n",
    "\n",
    "        # Decoderを通す\n",
    "        # out: [1, current_seq_len, d_model]\n",
    "        out = model.decode(ys, memory, src_mask, tgt_mask)\n",
    "\n",
    "        # 最後の時刻の出力を使って次の単語を予測\n",
    "        # prob: [1, vocab_size]\n",
    "        prob = model.fc_out(out[:, -1])\n",
    "\n",
    "        # 確率最大の単語IDを取得 (Greedy選択)\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        # 生成された単語をリストに追加\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "\n",
    "        # 終了トークン<eos>が生成されたら終了\n",
    "        if next_word == end_symbol:\n",
    "            break\n",
    "    \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyTaskDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, max_len: int, vocab_size: int):\n",
    "        \"\"\"ランダムな文字列のペア(src, tgt)を生成・保持するデータセットクラス\n",
    "        Args:\n",
    "            num_samples (int): 生成するサンプル数\n",
    "            max_len (int): 各サンプルの最大長\n",
    "            vocab_size (int): 単語IDの語彙数\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = self._generate_data()\n",
    "\n",
    "    def _generate_data(self) -> list:\n",
    "        data = []\n",
    "\n",
    "        # <sos>と<eos>のIDを定義\n",
    "        start_symbol = self.vocab_size\n",
    "        end_symbol = self.vocab_size + 1\n",
    "        for _ in range(self.num_samples):\n",
    "            # 1〜vocab_size-1 のランダムな数列(0はpad, vocab_size+2はstart/end token用に空けておく\n",
    "            seq_len = random.randint(1, self.max_len)\n",
    "\n",
    "            # ランダムな長さの数字列\n",
    "            seq = torch.randint(1, self.vocab_size, (seq_len,))\n",
    "\n",
    "            # Padding処理\n",
    "            # src: [seq_len] -> [max_len] に0埋め\n",
    "            src = torch.zeros(self.max_len + 2, dtype=torch.long)\n",
    "            src[:seq_len] = seq\n",
    "\n",
    "            # tgt: [<sos>, ..., <eos>] -> [max_len + 2] に0埋め\n",
    "            tgt = torch.zeros(self.max_len + 2, dtype=torch.long)\n",
    "            tgt[0] = start_symbol\n",
    "            tgt[1:seq_len + 1] = seq\n",
    "            tgt[seq_len + 1] = end_symbol\n",
    "            # 残りは0でパディングされたまま\n",
    "\n",
    "            data.append((src, tgt))\n",
    "        return data\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        # インデックスに対応する(src, tgt)ペアを返す\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch) -> tuple:\n",
    "    \"\"\"DataLoaderがミニバッチを作成する際に呼ばれるコールバック関数\n",
    "\n",
    "    Args:\n",
    "        batch (list): (src, tgt)ペアのリスト [(src1, tgt1), (src2, tgt2), ...]\n",
    "\n",
    "    Returns:\n",
    "        tuple: パディングされたsrcとtgtのテンソル\n",
    "    \"\"\"\n",
    "    # バッチ内のsrcとtgtのペアを分離\n",
    "    src_list, tgt_list = zip(*batch)\n",
    "\n",
    "    # リストをTensorに変換してスタック\n",
    "    # [batch_size, seq_len]になる\n",
    "    src_batch = torch.stack(src_list)\n",
    "    tgt_batch = torch.stack(tgt_list)\n",
    "\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ設定\n",
    "SRC_VOCAB = 100\n",
    "TGT_VOCAB = 100 + 2  # +2 for <sos> and <eos>\n",
    "D_MODEL = 128\n",
    "D_FF = 512\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 20\n",
    "\n",
    "# 学習設定\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "# 1. モデルの構築\n",
    "model = Transformer(\n",
    "    src_vocab_size=SRC_VOCAB, \n",
    "    tgt_vocab_size=TGT_VOCAB, \n",
    "    d_model=D_MODEL, \n",
    "    num_layers=NUM_LAYERS, \n",
    "    num_heads=NUM_HEADS, \n",
    "    d_ff=D_FF, \n",
    "    max_len=MAX_LEN + 2, # +2 for <sos> and <eos>\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# 重みの初期化 Xavier Initialization\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "# 2. オプティマイザとスケジューラ\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = NoamScheduler(optimizer=optimizer, d_model=D_MODEL, warmup_steps=1000)\n",
    "\n",
    "# 3. 損失関数 Label Smoothing Cross Entropy\n",
    "# pad_idx=0を無視するように設定\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.0)\n",
    "\n",
    "# 4. ダミーデータの生成とデータローダの作成\n",
    "dataset = CopyTaskDataset(num_samples=1000, max_len=MAX_LEN, vocab_size=SRC_VOCAB)\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn, # カスタムコールバック関数(バッチ化の処理)を指定\n",
    "    drop_last=True # 最後の端数バッチが小さい場合に破棄(サイズを固定したい場合)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 学習ループ実行\n",
    "print(\"Start training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train_epoch(model, data_loader, scheduler, criterion, DEVICE)\n",
    "    print(f\"Epoch: {epoch+1: 02} | Loss: {loss:.4f} | LR: {scheduler.get_lr():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4e8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 推論の確認\n",
    "print(\"Start inference...\")\n",
    "model.eval()\n",
    "test_src = torch.tensor([[1, 2, 3, 4, 5]]).to(DEVICE)  # [1, src_len]\n",
    "src_mask = create_padding_mask(test_src, pad_idx=0).to(DEVICE)\n",
    "\n",
    "# 推論実行\n",
    "# start_symbol=100, end_symbol=101を指定\n",
    "generated = greedy_decode(model, test_src, src_mask, max_len=10, start_symbol=100, end_symbol=101, device=DEVICE)\n",
    "print(f\"Input Sequence: {test_src.cpu().numpy()}\")\n",
    "print(f\"Generated Sequence: {generated.cpu().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
