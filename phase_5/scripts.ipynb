{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43950d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"パディングマスクを作成する関数。<pad>の部分を1e-9、それ以外を0にする。\n",
    "\n",
    "    Args:\n",
    "        seq (torch.Tensor): [batch_size, seq_len]の形状を持つテンソル。入力単語のID列\n",
    "        pad_idx (int, optional): パディングを表すID。 Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [batch_size, 1, 1, seq_len] Mult-head Attentionのスコアに加算するためのマスク\n",
    "    \"\"\"\n",
    "\n",
    "    # seq == pad_idx の部分はTrue、それ以外はFalse\n",
    "    mask = (seq == pad_idx)\n",
    "\n",
    "    # shapeを [batch_size, 1, 1, seq_len] に変形\n",
    "    # Trueを1e-9、Falseを0に変換\n",
    "    # float型に変換しないと加算時にエラーになる\n",
    "    return mask.unsqueeze(1).unsqueeze(2).float() * -1e9\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"未来の単語を見えなくするための上三角マスクを作成する関数\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): シーケンス長\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [1, 1, seq_len, seq_len] 対角成分より上が1e-9、それ以外が0の行列\n",
    "    \"\"\"\n",
    "    # torch.triuで上三角行列を取り出す (diagonal=1で対角線のひとつ上から)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "    # マスク部分を1e-9、それ以外を0に変換\n",
    "    return mask.unsqueeze(0).unsqueeze(0).float() * -1e9\n",
    "\n",
    "\n",
    "def create_masks(src: torch.Tensor, tgt: torch.Tensor, pad_idx: int = 0):\n",
    "    \"\"\"EncoderとDecoderに必要なすべてのマスクを一括生成するヘルパー関数\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): [batch_size, src_len]\n",
    "        tgt (torch.Tensor): [batch_size, tgt_len]\n",
    "        pad_idx (int, optional): パディングID。 Defaults to 0.\n",
    "    \"\"\"\n",
    "    # 1. Encoder用のマスク(Padding Maskのみ)\n",
    "    src_mask = create_padding_mask(src, pad_idx)\n",
    "\n",
    "    # 2. Decoder用のマスク(Padding Mask + Look-ahead Mask)\n",
    "    # 2.1. TargetのPadding Mask [batch_size, 1, 1, tgt_len]\n",
    "    tgt_pad_mask = create_padding_mask(tgt, pad_idx)\n",
    "\n",
    "    # 2.2. Look-ahead Mask [1, 1, tgt_len, tgt_len]\n",
    "    tgt_len = tgt.size(1)\n",
    "    look_ahead_mask = create_look_ahead_mask(tgt_len).to(tgt.device)\n",
    "\n",
    "    # 2.3. Padding MaskとLook-ahead Maskを結合 (どちらかが-1e9なら-1e9になるように加算またはmaxを取る)\n",
    "    # ここで単純に和を取ると-2e9になる箇所ができるが、Softmaxにおいては十分小さいので計算には影響しない\n",
    "    # 論理和(OR)的にマスクしたいので、最小値を取る実装もよくある(torch.min)\n",
    "    tgt_mask = torch.min(tgt_pad_mask, look_ahead_mask)\n",
    "\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, d_model: int, warmup_steps: int = 4000):\n",
    "        \"\"\"Transformer用学習率スケジューラ (Noam Scheduler)\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): PytorchのOptimizer\n",
    "            d_model (int): モデルの次元数\n",
    "            warmup_steps (int, optional): ウォームアップステップ数。 Defaults to 4000.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"1ステップ進め、学習率を更新し、optimizer.step()を実行する\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "\n",
    "        # Optimizerの学習率を更新\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        \"\"\"現在のステップ数に基づき学習率を計算\n",
    "\n",
    "        Returns:\n",
    "            float: 学習率\n",
    "        \"\"\"\n",
    "        step = self.current_step\n",
    "        return (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"optimizerの勾配を初期化(ゼロにする)\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_masks():\n",
    "    src_len = 5\n",
    "    tgt_len = 5\n",
    "\n",
    "    # ダミーデータ: 0はpad_idxとする\n",
    "    # src: [1, 2, 3, 0, 0] 長さ3、pad2\n",
    "    src = torch.tensor([[1, 2, 3, 0, 0]])\n",
    "    # tgt: [1, 2, 3, 4, 0] 長さ4、pad1\n",
    "    tgt = torch.tensor([[1, 2, 3, 4, 0]])\n",
    "\n",
    "    src_mask, tgt_mask = create_masks(src, tgt, pad_idx=0)\n",
    "\n",
    "    print(f\"Sorce Mask Shape: {src_mask.shape}\")\n",
    "    print(f\"Target Mask Shape: {tgt_mask.shape}\")\n",
    "\n",
    "    # グラフ描画\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Source Mask (Paddingのみ)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # [1, 1, 5] -> [1, 5] 表示のため次元削減\n",
    "    sns.heatmap(src_mask[0, 0, 0, :].unsqueeze(0).numpy(), cbar=False, square=True)\n",
    "    plt.title(\"Source Mask (Padding)\")\n",
    "\n",
    "    # Target Mask (Padding + Look-ahead)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # [1, 5, 5]\n",
    "    sns.heatmap(tgt_mask[0, 0, :, :].numpy(), cbar=False, square=True)\n",
    "    plt.title(\"Target Mask (Padding + Look-ahead)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_scheduler():\n",
    "    # ダミーモデルとオプティマイザ\n",
    "    model = torch.nn.Linear(10, 10)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0) # 初期lrは0でOK\n",
    "\n",
    "    d_model = 512\n",
    "    warmup_steps = 4000\n",
    "    scheduler = NoamScheduler(optimizer, d_model, warmup_steps)\n",
    "\n",
    "    lrs = []\n",
    "    for _ in range(20000):\n",
    "        scheduler.current_step += 1\n",
    "        lrs.append(scheduler.get_lr())\n",
    "    \n",
    "    plt.plot(lrs)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Noam Scheduler\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "visualize_scheduler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
