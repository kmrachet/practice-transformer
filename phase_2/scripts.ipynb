{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): モデルの隠れ層の次元数\n",
    "            num_heads (int): ヘッドの数\n",
    "            dropout (float, optional): ドロップアウト率. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # d_modelがnum_headsで割り切れることを確認\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # 各ヘッドの次元数\n",
    "\n",
    "        # Q, K, Vの線形変換\n",
    "        # 実際には全ヘッド分を一度に計算するため、出力次元はd_modelのまま\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): [batch_size, seq_len, d_model]\n",
    "            key (torch.Tensor):   [batch_size, seq_len, d_model]\n",
    "            value (torch.Tensor): [batch_size, seq_len, d_model]\n",
    "            mask (torch.Tensor, optional): [batch_size, 1, 1, seq_len] または [batch_size, 1, seq_len, seq_len]\n",
    "                                           (0: マスクなし、1: マスクありなどの定義によるが、ここでは加算マスクを想定)\n",
    "                                           Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. 線形変換\n",
    "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, num_heads]\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "\n",
    "        # 2. ヘッドの分割\n",
    "        # [batch_size, seq_len, num_heads] -> [batch_size, seq_len, num_heads, d_k]\n",
    "        # その後、計算しやすいようにヘッドの次元を先頭に移動させる(転置) -> [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 3. Scaled Dot-Product Attention\n",
    "        # 3.1. スコアの計算 Q * K^T / sqrt(d_k)\n",
    "        # Q: [..., seq_len_q, d_k], K^T: [..., d_k, seq_len_k] -> scores: [..., seq_len_q, seq_len_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 3.2. マスクの適用(optional)\n",
    "        if mask is not None:\n",
    "            # ここではマスクが0の場所を非常に小さい値(-1e9)でマスクすると仮定\n",
    "            # 実装により1と0の定義が異なる場合があるため注意する\n",
    "            # 非常に小さい値(-1e9)で埋めることで、softmax後にほぼ0になるようにする\n",
    "            # scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            scores = scores + mask # 加算マスクの場合 (maskが0の場所に-1e9が入っている想定)\n",
    "        \n",
    "        # 3.3. softmax & dropout\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 3.4. Valueとの積\n",
    "        # attention_weights: [..., seq_len_q, seq_len_k] * V: [..., seq_len_k, d_k] -> [...,seq_lenn_q, d_k]\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        # 4. ヘッドの結合\n",
    "        # [batch_size, num_beads, seq_len, d_k] -> [batch_size, seq_len, num_heads, d_k]\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # [batch_size, seq_len, d_model]に戻す\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 5. 線形変換\n",
    "        output = self.fc_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_multi_head_attention():\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # ダミー入力の作成\n",
    "    # self-attentionなのでkey, valueもqueryと同じにする\n",
    "    x = torch.rand(batch_size, seq_len, d_model)\n",
    "    output = mha(x, x, x)\n",
    "\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # 出力の形状が入力と同じであることを確認\n",
    "    assert output.shape == x.shape, \"Output shape must match input shape\"\n",
    "    print(\"MultiHeadAttention verification passed!\")\n",
    "\n",
    "verify_multi_head_attention()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
